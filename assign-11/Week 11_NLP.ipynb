{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350265fd",
   "metadata": {},
   "source": [
    "# **Week 11 – Natural Language Processing (NLP)**\n",
    "\n",
    "This week introduces core Natural Language Processing techniques using both deep learning (word embeddings + LSTM) and traditional ML preprocessing (stopwords, tokenization, lemmatization, TF-IDF).\n",
    "\n",
    "Since the main house price dataset does not contain text, the Class Task uses a sample text dataset **(IMDB Movie Reviews)**. Assignment 11 follows two paths:\n",
    "\n",
    "- If your project dataset contains text → apply full NLP preprocessing\n",
    "\n",
    "- If not (like house prices) → demonstrate NLP workflows on a small sample dataset\n",
    "\n",
    "This approach ensures you learn NLP techniques even if your project dataset is tabular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39d5ac",
   "metadata": {},
   "source": [
    "# **Class Task – Tokenization & Word Embeddings (IMDB Dataset)**\n",
    "\n",
    "In the Class Task, I implemented tokenization and deep learning–based word embeddings using the IMDB Movie Reviews dataset. The steps included:\n",
    "\n",
    "- Loading the dataset as sequences of integer tokens\n",
    "\n",
    "- Padding sequences to a fixed length\n",
    "\n",
    "- Building a model using an Embedding layer + LSTM\n",
    "\n",
    "- Training and evaluating a sentiment classifier\n",
    "\n",
    "This task helped me understand how raw text is transformed into structured numerical representations using tokenization and embeddings. I also observed how LSTMs capture contextual meaning and sequential patterns within text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d458bc5",
   "metadata": {},
   "source": [
    "**Step 1: Load IMDB Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8240b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 25000\n",
      "Test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load top 10,000 most frequent words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Test samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e7ad5",
   "metadata": {},
   "source": [
    "**Step 2: Pad Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ea8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200  # sequence length after padding\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0c5e0",
   "metadata": {},
   "source": [
    "**Step 3: Build Simple Embedding + LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef9e2b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naree\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=64, input_length=max_len),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb303c67",
   "metadata": {},
   "source": [
    "**Step 4: Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ea09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 304ms/step - accuracy: 0.7373 - loss: 0.4990 - val_accuracy: 0.8472 - val_loss: 0.3575\n",
      "Epoch 2/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 273ms/step - accuracy: 0.8921 - loss: 0.2670 - val_accuracy: 0.8658 - val_loss: 0.3165\n",
      "Epoch 3/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 254ms/step - accuracy: 0.9270 - loss: 0.1944 - val_accuracy: 0.8464 - val_loss: 0.3487\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411a4dd",
   "metadata": {},
   "source": [
    "**Step 5: Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28adcda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8504 - loss: 0.3464\n",
      "Test Accuracy: 0.8503599762916565\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa214d00",
   "metadata": {},
   "source": [
    "# **Assignment 11 – NLP Preprocessing (Stopwords, Tokenization, Lemmatization, TF-IDF)**\n",
    "\n",
    "Although the main project dataset (house prices) is tabular and lacks text content, I implemented a separate NLP preprocessing pipeline using a small sample text corpus (IMDB reviews). The steps included:\n",
    "\n",
    "- Lowercasing and punctuation removal\n",
    "\n",
    "- Tokenization (splitting text into words)\n",
    "\n",
    "- Stop-word removal\n",
    "\n",
    "- Lemmatization\n",
    "\n",
    "- TF-IDF vectorization\n",
    "\n",
    "This exercise demonstrates my understanding of standard NLP workflows. It also clarifies why such methods are not applicable to the current project: without text data (e.g., descriptions, reviews, or time-series), textual feature engineering is not feasible.\n",
    "\n",
    "If in the future the project includes textual data such as house descriptions or seller notes, this exact pipeline can be directly used to extract meaningful features for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac3192",
   "metadata": {},
   "source": [
    "**Step 1: Load IMDB Raw Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060bdf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
       "1  Homelessness (or Houselessness as George Carli...      1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
       "3  This is easily the most underrated film inn th...      1\n",
       "4  This is not the typical Mel Brooks film. It wa...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1.tar.gz\",\n",
    "    \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    untar=True,\n",
    "    cache_dir='.',\n",
    ")\n",
    "\n",
    "folder = \"./datasets/aclImdb_v1_extracted/aclImdb\"\n",
    "\n",
    "# Load train text files\n",
    "import os\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "for label in [\"pos\", \"neg\"]:\n",
    "    labeled_dir = os.path.join(folder, \"train\", label)\n",
    "    for file in os.listdir(labeled_dir):\n",
    "        with open(os.path.join(labeled_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            train_texts.append(f.read())\n",
    "            train_labels.append(1 if label == \"pos\" else 0)\n",
    "\n",
    "df = pd.DataFrame({\"text\": train_texts, \"label\": train_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e60776",
   "metadata": {},
   "source": [
    "**Step 2: Clean Text (Lowercase, Remove symbols)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee365a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness or houselessness as george carlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant overacting by lesley ann warren best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is not the typical mel brooks film it was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1   \n",
       "1  Homelessness (or Houselessness as George Carli...      1   \n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1   \n",
       "3  This is easily the most underrated film inn th...      1   \n",
       "4  This is not the typical Mel Brooks film. It wa...      1   \n",
       "\n",
       "                                               clean  \n",
       "0  bromwell high is a cartoon comedy it ran at th...  \n",
       "1  homelessness or houselessness as george carlin...  \n",
       "2  brilliant overacting by lesley ann warren best...  \n",
       "3  this is easily the most underrated film inn th...  \n",
       "4  this is not the typical mel brooks film it was...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean\"] = df[\"text\"].apply(clean_text)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4edb3",
   "metadata": {},
   "source": [
    "**Step 3: Tokenization + Stopword Removal + Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7918b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\naree\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness or houselessness as george carlin...</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant overacting by lesley ann warren best...</td>\n",
       "      <td>brilliant overacting lesley ann warren best dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is not the typical mel brooks film it was...</td>\n",
       "      <td>typical mel brook film much less slapstick mov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...      1   \n",
       "1  Homelessness (or Houselessness as George Carli...      1   \n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...      1   \n",
       "3  This is easily the most underrated film inn th...      1   \n",
       "4  This is not the typical Mel Brooks film. It wa...      1   \n",
       "\n",
       "                                               clean  \\\n",
       "0  bromwell high is a cartoon comedy it ran at th...   \n",
       "1  homelessness or houselessness as george carlin...   \n",
       "2  brilliant overacting by lesley ann warren best...   \n",
       "3  this is easily the most underrated film inn th...   \n",
       "4  this is not the typical mel brooks film it was...   \n",
       "\n",
       "                                           processed  \n",
       "0  bromwell high cartoon comedy ran time program ...  \n",
       "1  homelessness houselessness george carlin state...  \n",
       "2  brilliant overacting lesley ann warren best dr...  \n",
       "3  easily underrated film inn brook cannon sure f...  \n",
       "4  typical mel brook film much less slapstick mov...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"processed\"] = df[\"clean\"].apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92698a",
   "metadata": {},
   "source": [
    "**Step 4: TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0a2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X = tfidf.fit_transform(df[\"processed\"]).toarray()\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17d7cb",
   "metadata": {},
   "source": [
    "**Step 5: Train Logistic Regression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed8523aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model Accuracy: 0.8792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model_tfidf = LogisticRegression(max_iter=200)\n",
    "model_tfidf.fit(X_train, y_train)\n",
    "\n",
    "pred = model_tfidf.predict(X_test)\n",
    "print(\"TF-IDF Model Accuracy:\", accuracy_score(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
